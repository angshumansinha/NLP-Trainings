{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f419a44",
   "metadata": {},
   "source": [
    "<h1> <center> BASIC TWITTER SENTIMENTAL ANALYSIS 01 </h1> </center><br> we will look at a basic nlp case uing the tweepy api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cddfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.26.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.1 requests-oauthlib-1.3.0 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "#lets first install tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e504731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "#we need textblob to process textual data \n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b7e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "#for having a predefined large and structured set of texts\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cc493",
   "metadata": {},
   "source": [
    "<h1> <center> Let's get started </h1> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ec523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycountry\n",
      "  Downloading pycountry-20.7.3.tar.gz (10.1 MB)\n",
      "Building wheels for collected packages: pycountry\n",
      "  Building wheel for pycountry (setup.py): started\n",
      "  Building wheel for pycountry (setup.py): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=ced7a9159398e5e112d7bd52ed04f3d7731615b79731b65bf3f949164d8e7624\n",
      "  Stored in directory: c:\\users\\angsh\\appdata\\local\\pip\\cache\\wheels\\09\\eb\\0d\\4ee773c6a4aadc2a43cb5c1d07f268f13c4cdc0eec88e7c1ef\n",
      "Successfully built pycountry\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-20.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52621a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=cf3fede976a9aa211e54c5f8aee82a35726e1bbc54ec22e794f606e9b7b20f6c\n",
      "  Stored in directory: c:\\users\\angsh\\appdata\\local\\pip\\cache\\wheels\\13\\c7\\b0\\79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a738e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c1310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets understand the basic structure\n",
    "#Let us get our keys and tokens\n",
    "api_key=\"2a8lghLsyhYZyUe03GH4GwoGx\"\n",
    "api_secret_key=\"hiWv0xCtJMJMvhJQ4Q7pykFSrFICGEoUMG0ChDL4a7iMJdBktS\"\n",
    "access_token=\"1379382616848097284-oNOccgtpESDh6FRsMSPaGItgyFvZLI\"\n",
    "access_token_secret=\"cmC40qvOXaNfSaUIafQPSWw0meQzLMCDeiTTV9s6i9Lto\"\n",
    "\n",
    "#authorise\n",
    "authorise=tweepy.OAuthHandler(api_key,api_secret_key)\n",
    "authorise.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "#finally setup the api\n",
    "api=tweepy.API(authorise)\n",
    "public_tweets=api.search(\"Samsung\") #list of tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edacbd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best banget lah samsung #GalaxyA22 ini. Sinyal 5G ngacir buat streaming drakor #SuperAwesome\n",
      "Sentiment(polarity=1.0, subjectivity=0.3)\n",
      "RT @TSUKUMOofficial: ğŸ‰ã€ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆä¼ç”»ã€‘ğŸ‰\n",
      "è¶…é«˜é€Ÿã®æ–°ä¸–ä»£NVMe SSD\n",
      "\n",
      "Samsung SSD 980 PRO (2TB/PCIe Gen4 x4)\n",
      "ã‚’1åæ§˜ã«ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ!\n",
      "\n",
      "å¿œå‹Ÿæ–¹æ³•ï¼šãƒ„ã‚¯ãƒ¢å…¬å¼(@TSUKUMOofficial)ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼RTâ€¦\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "nah why they sound like they using twt on a samsung fridge\n",
      "Sentiment(polarity=0.4, subjectivity=0.4)\n",
      "RT @Electronynet: Ø³Ø§Ù…Ø³ÙˆÙ†Ø¬ ØªØ´ÙˆÙ‚ Ù„Ù„ÙƒØ´Ù Ø¹Ù† ØªÙØ§ØµÙŠÙ„ ÙˆÙ…ÙˆØ§ØµÙØ§Øª Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù…Ù† Ø´Ø§Ø´Ø© Odyssey Neo G9 ÙÙŠ 29 ÙŠÙˆÙ„ÙŠÙˆ Ø§Ù„Ù‚Ø§Ø¯Ù… ğŸ–¥ï¸\n",
      "\n",
      "#Samsung https://t.co/7Mâ€¦\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @TSUKUMOofficial: ğŸ‰ã€ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆä¼ç”»ã€‘ğŸ‰\n",
      "è¶…é«˜é€Ÿã®æ–°ä¸–ä»£NVMe SSD\n",
      "\n",
      "Samsung SSD 980 PRO (2TB/PCIe Gen4 x4)\n",
      "ã‚’1åæ§˜ã«ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ!\n",
      "\n",
      "å¿œå‹Ÿæ–¹æ³•ï¼šãƒ„ã‚¯ãƒ¢å…¬å¼(@TSUKUMOofficial)ã‚’ãƒ•ã‚©ãƒ­ãƒ¼ï¼RTâ€¦\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @PigiaMe: Samsung Buds Pro:https://t.co/044RyKv8Zt\n",
      "[KSh 15,999]\n",
      "â˜‘ Active Noise-cancellation\n",
      "â˜‘ 61MAh Battery Capacity\n",
      "â˜‘ 472MAh Case Capacâ€¦\n",
      "Sentiment(polarity=-0.13333333333333333, subjectivity=0.6)\n",
      "RT @gogoogleandroid: Spec comparison of Snapdragon 888 vs Dimensity 1200. \n",
      "\n",
      "ğŸ˜ğŸ˜ğŸ’Ÿ #snapdragon895 #snapdragon888 #geekbench #snapdragon #qualcâ€¦\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @christaline_: GIVEAWAY $150 | 2.100.000 | 7200 PHP\n",
      "SAMSUNG GALAXY M12 for 1 winner âœ¨\n",
      "\n",
      "â€¢ RT + follow me &amp; \n",
      "\n",
      "@rmastro9424 (ğŸ””)\n",
      "\n",
      "end in 72â€¦\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@madeiradez Ah, uso o Samsung notes (habilitei sincronia do iPhone para Google e puxei quase todas as minhas notasâ€¦ https://t.co/1rBReDBTwU\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@selalujae kata nya samsung s9 ğŸ˜­ğŸ˜­\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@MayarWaluigi @GamePl123 @mariokarttourEN I saw one guy has Samsung J3+ and his phone pretty ok to support for Mariâ€¦ https://t.co/zJZboUY8Gr\n",
      "Sentiment(polarity=0.375, subjectivity=0.75)\n",
      "Â¡Nueva oferta a la vista!\n",
      "\n",
      "ğŸ“¦ EMK - 0,5M Cable de extensiÃ³n de cable Ã³ptico de audio Toslink hembra a macho, compatiâ€¦ https://t.co/B3stLdNm7F\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @IM3Ooredoo: Siapa yang mau dapet GIVEAWAY Samsung Galaxy A21? ğŸ™ŒğŸ»\n",
      "\n",
      "Ikutin kuisnya dan cari jawabannya di ğŸ‘‰ğŸ» https://t.co/AKvT6MunZsâ€¦\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "samsung seeing jungkookâ€™s iphone 12 https://t.co/WYxlmhvjPw\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@SamsungMobile @SamsungMobileUS @Samsung whoever this concerns y'all been giving me the run around for months and Iâ€¦ https://t.co/rIzzwaSJBI\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "#display tweets\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n",
    "    analysis=TextBlob(tweet.text)\n",
    "    print(analysis.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9032eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sentiment scores depend on how clean and processed the data is\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
