{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f419a44",
   "metadata": {},
   "source": [
    "<h1> <center> BASIC TWITTER SENTIMENTAL ANALYSIS 01 </h1> </center><br> we will look at a basic nlp case uing the tweepy api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cddfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-3.10.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.26.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.1.1 requests-oauthlib-1.3.0 tweepy-3.10.0\n"
     ]
    }
   ],
   "source": [
    "#lets first install tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e504731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "#we need textblob to process textual data \n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b7e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\angsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "#for having a predefined large and structured set of texts\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cc493",
   "metadata": {},
   "source": [
    "<h1> <center> Let's get started </h1> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ec523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycountry\n",
      "  Downloading pycountry-20.7.3.tar.gz (10.1 MB)\n",
      "Building wheels for collected packages: pycountry\n",
      "  Building wheel for pycountry (setup.py): started\n",
      "  Building wheel for pycountry (setup.py): finished with status 'done'\n",
      "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=ced7a9159398e5e112d7bd52ed04f3d7731615b79731b65bf3f949164d8e7624\n",
      "  Stored in directory: c:\\users\\angsh\\appdata\\local\\pip\\cache\\wheels\\09\\eb\\0d\\4ee773c6a4aadc2a43cb5c1d07f268f13c4cdc0eec88e7c1ef\n",
      "Successfully built pycountry\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-20.7.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52621a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in c:\\users\\angsh\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=cf3fede976a9aa211e54c5f8aee82a35726e1bbc54ec22e794f606e9b7b20f6c\n",
      "  Stored in directory: c:\\users\\angsh\\appdata\\local\\pip\\cache\\wheels\\13\\c7\\b0\\79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a738e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98c1310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets understand the basic structure\n",
    "#Let us get our keys and tokens\n",
    "api_key=\"2a8lghLsyhYZyUe03GH4GwoGx\"\n",
    "api_secret_key=\"hiWv0xCtJMJMvhJQ4Q7pykFSrFICGEoUMG0ChDL4a7iMJdBktS\"\n",
    "access_token=\"1379382616848097284-oNOccgtpESDh6FRsMSPaGItgyFvZLI\"\n",
    "access_token_secret=\"cmC40qvOXaNfSaUIafQPSWw0meQzLMCDeiTTV9s6i9Lto\"\n",
    "\n",
    "#authorise\n",
    "authorise=tweepy.OAuthHandler(api_key,api_secret_key)\n",
    "authorise.set_access_token(access_token,access_token_secret)\n",
    "\n",
    "#finally setup the api\n",
    "api=tweepy.API(authorise)\n",
    "public_tweets=api.search(\"Samsung\") #list of tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edacbd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best banget lah samsung #GalaxyA22 ini. Sinyal 5G ngacir buat streaming drakor #SuperAwesome\n",
      "Sentiment(polarity=1.0, subjectivity=0.3)\n",
      "RT @TSUKUMOofficial: 🎉【プレゼント企画】🎉\n",
      "超高速の新世代NVMe SSD\n",
      "\n",
      "Samsung SSD 980 PRO (2TB/PCIe Gen4 x4)\n",
      "を1名様にプレゼント!\n",
      "\n",
      "応募方法：ツクモ公式(@TSUKUMOofficial)をフォロー／RT…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "nah why they sound like they using twt on a samsung fridge\n",
      "Sentiment(polarity=0.4, subjectivity=0.4)\n",
      "RT @Electronynet: سامسونج تشوق للكشف عن تفاصيل ومواصفات الجيل الجديد من شاشة Odyssey Neo G9 في 29 يوليو القادم 🖥️\n",
      "\n",
      "#Samsung https://t.co/7M…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @TSUKUMOofficial: 🎉【プレゼント企画】🎉\n",
      "超高速の新世代NVMe SSD\n",
      "\n",
      "Samsung SSD 980 PRO (2TB/PCIe Gen4 x4)\n",
      "を1名様にプレゼント!\n",
      "\n",
      "応募方法：ツクモ公式(@TSUKUMOofficial)をフォロー／RT…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @PigiaMe: Samsung Buds Pro:https://t.co/044RyKv8Zt\n",
      "[KSh 15,999]\n",
      "☑ Active Noise-cancellation\n",
      "☑ 61MAh Battery Capacity\n",
      "☑ 472MAh Case Capac…\n",
      "Sentiment(polarity=-0.13333333333333333, subjectivity=0.6)\n",
      "RT @gogoogleandroid: Spec comparison of Snapdragon 888 vs Dimensity 1200. \n",
      "\n",
      "😍😍💟 #snapdragon895 #snapdragon888 #geekbench #snapdragon #qualc…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @christaline_: GIVEAWAY $150 | 2.100.000 | 7200 PHP\n",
      "SAMSUNG GALAXY M12 for 1 winner ✨\n",
      "\n",
      "• RT + follow me &amp; \n",
      "\n",
      "@rmastro9424 (🔔)\n",
      "\n",
      "end in 72…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@madeiradez Ah, uso o Samsung notes (habilitei sincronia do iPhone para Google e puxei quase todas as minhas notas… https://t.co/1rBReDBTwU\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@selalujae kata nya samsung s9 😭😭\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@MayarWaluigi @GamePl123 @mariokarttourEN I saw one guy has Samsung J3+ and his phone pretty ok to support for Mari… https://t.co/zJZboUY8Gr\n",
      "Sentiment(polarity=0.375, subjectivity=0.75)\n",
      "¡Nueva oferta a la vista!\n",
      "\n",
      "📦 EMK - 0,5M Cable de extensión de cable óptico de audio Toslink hembra a macho, compati… https://t.co/B3stLdNm7F\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "RT @IM3Ooredoo: Siapa yang mau dapet GIVEAWAY Samsung Galaxy A21? 🙌🏻\n",
      "\n",
      "Ikutin kuisnya dan cari jawabannya di 👉🏻 https://t.co/AKvT6MunZs…\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "samsung seeing jungkook’s iphone 12 https://t.co/WYxlmhvjPw\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "@SamsungMobile @SamsungMobileUS @Samsung whoever this concerns y'all been giving me the run around for months and I… https://t.co/rIzzwaSJBI\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "#display tweets\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)\n",
    "    analysis=TextBlob(tweet.text)\n",
    "    print(analysis.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9032eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the sentiment scores depend on how clean and processed the data is\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
